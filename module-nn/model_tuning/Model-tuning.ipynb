{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DeapSECURE module 4: Deap Learning**\n",
    "\n",
    "# Session 3: Model Tuning\n",
    "\n",
    "Welcome to the DeapSECURE online training program!\n",
    "This is a Jupyter notebook for the hands-on learning activities of the\n",
    "[\"Deep Learning\" (DL) module](https://deapsecure.gitlab.io/deapsecure-lesson04-nn/),\n",
    "Episode 6: [\"Tuning Neural Network Models for Better Accuracy\"](https://deapsecure.gitlab.io/deapsecure-lesson04-nn/30-model-tuning/index.html).\n",
    "Please visit the [DeapSECURE](https://deapsecure.gitlab.io/) website to learn more about our training program.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this session, we will use this notebook to **tune** neural network models to improve the accuracy of the classification task on the Sherlock's \"Applications\" dataset.\n",
    "We will be using the same dataset, which contains 18 applications.\n",
    "\n",
    "> **Your challenge** in this notebook is to train more neural network models using the \"18-apps\" dataset to improve the classification accuracy of the model. Can we reach 99%? How about 99.9%? Or 99.99%?\n",
    "\n",
    "> **DISCUSSION**: In cybersecurity, why do we care about 99.99% or even 99.999% accuracy?\n",
    "> Think, for example, the case of spam detection.\n",
    "> What will happen if we falsely mark many legitimate emails as spam?\n",
    "> Or let many spam mails enter into your inbox?\n",
    "\n",
    "**QUICK LINKS**\n",
    "* [Setup](#sec-setup)\n",
    "* [Loading Sherlock Applications Data](#sec-load_data)\n",
    "* [Neural Network Models](#sec-NN)\n",
    "* [Model Tuning Methods](#sec-Model_Tuning_Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-setup\"></a>\n",
    "## 1. Setup Instructions\n",
    "\n",
    "If you are opening this notebook from the Wahab OnDemand interface, you're all set.\n",
    "\n",
    "If you see this notebook elsewhere, and want to perform the exercises on Wahab cluster, please follow the steps outlined in our setup procedure.\n",
    "\n",
    "1. Make sure you have activated your HPC service.\n",
    "2. Point your web browser to https://ondemand.wahab.hpc.odu.edu/ and sign in with your MIDAS ID and password.\n",
    "3. Create a new Jupyter session with the following parameters: Python version **3.7**, Python suite `tensorflow 2.6 + pytorch 1.10`, Number of Cores **4**, Number of GPU **0**, Partition `main`, and Number of Hours at least **4**. (See <a href=\"https://wiki.hpc.odu.edu/en/ood-jupyter\" target=\"_blank\">ODU HPC wiki</a> for more detailed help.)\n",
    "4. From the JupyterLab launcher, start a new Terminal session. Then issue the following commands to get the necessary files:\n",
    "\n",
    "       mkdir -p ~/CItraining/module-nn\n",
    "       cp -pr /shared/DeapSECURE/module-nn/. ~/CItraining/module-nn\n",
    "\n",
    "Using the file manager on the left sidebar, now change the working directory to `~/CItraining/module-nn`.\n",
    "The file name of this notebook is `NN-session-3.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reminder\n",
    "\n",
    "* Throughout this notebook, `#TODO` is used as a placeholder where you need to fill in with something appropriate. \n",
    "\n",
    "* To run a code in a cell, press `Shift+Enter`.\n",
    "\n",
    "* <a href=\"https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\" target=\"_blank\">Pandas cheatsheet</a>\n",
    "\n",
    "* <a href=\"https://deapsecure.gitlab.io/deapsecure-lesson02-bd/10-pandas-intro/index.html#summary-indexing-syntax\" target=\"_blank\">Summary table of the commonly used indexing (subscripting) syntax</a> from our own lesson.\n",
    "\n",
    "* <a href=\"https://keras.io/api/\" target=\"_blank\">Keras API document</a>\n",
    "\n",
    "We recommend you open these on separate tabs or print them;\n",
    "they are handy help for writing your own codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading Python Libraries\n",
    "\n",
    "First step, we need to import the required libraries into this Jupyter Notebook:\n",
    "`pandas`, `numpy`,`matplotlib.pyplot`, and `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CUSTOMIZATIONS (optional)\n",
    "np.set_printoptions(linewidth=1000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools for deep learning:\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Import key Keras objects\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML toolbox functions\n",
    "from sherlock_ML_toolbox import load_prep_data_18apps, split_data_18apps, \\\n",
    "NN_Model_1H, plot_loss, plot_acc, combine_loss_acc_plots, fn_out_history_1H, \\\n",
    "model_layer_code_XH, fn_dir_tuning_XH, fn_out_history_XH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-load_data\"></a>\n",
    "## 2. Loading Sherlock Applications Data\n",
    "\n",
    "Utilize the toolbox sherlock_ML_toolbox.py to load in the data, \n",
    "preprocess the data (data cleaning, label/feature separation,\n",
    "feature normalization/scaling, etc.) until it is ready for ML except\n",
    "for train-validation splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the pre-processed SherLock data.\n",
    "datafile = \"sherlock/sherlock_18apps.csv\"\n",
    "df_orig, df, labels, df_labels_onehot, df_features \\\n",
    "    = #TODO\n",
    "\n",
    "# Split the data into train and validation datasets with their respective features and labels.\n",
    "train_features, val_features, train_labels, val_labels, train_L_onehot, val_L_onehot \\\n",
    "    = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 10 rows/entries from the preprocessed data:\")\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last 10 rows/entries from the preprocessed data:\")\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-NN\"></a>\n",
    "## 3. Neural Network Model with One Hidden Layer -- the Baseline\n",
    "\n",
    "Let us now start by building a simple neural network model with just one hidden layer.\n",
    "This will serve as a *baseline*, which we will attempt to improve through the tuning process below.\n",
    "\n",
    "### Aside: Metadata\n",
    "\n",
    "(Experiment) metadata: provides context about each run/experiment.\n",
    "Saving metadata provides the user with a quick way to recall \n",
    "important information about a particular run/experiment.\n",
    "\n",
    "In these experiments, we will save the following metadata:\n",
    " - Expt_ID: shorthand of the naming convention along with the type of experiment\n",
    " - Job_ID: this will be unique for each experiment\n",
    " - hidden_neurons: as a list, where each element is the number of hidden neurons in that layer\n",
    " - learning_rate\n",
    " - batch_size\n",
    " \n",
    "This metadata can either be saved during each experiment (this helps ensure that no mistakes are made);\n",
    "or, it can be saved after if the user is very careful to remember\n",
    "what to fill in for each run.\n",
    "\n",
    "Since these experiments very methodically scan the hyperparameter\n",
    "space, we will collect the metadata at the end of each experiment type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Baseline Model\n",
    "\n",
    "The baseline neural network model has one hidden layer with `18` hidden neurons and a learning rate of `0.0003`. It is trained with a batch size of 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train this model with an initial *learning rate* of 0.0003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the outer hidden_neurons directory\n",
    "dir0_HN = \"scan-hidden-neurons/\"\n",
    "if not os.path.exists(dir0_HN):\n",
    "        os.makedirs(dir0_HN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function\n",
    "\n",
    "def saveOutputs_HN(numNeurons, currHistory, currModel):\n",
    "    \"\"\"\n",
    "    Save the outputs of the hidden neurons model tuning experiments.\n",
    "    It will create a directory within the hidden neurons directory with the \n",
    "    MODEL_DIR name.\n",
    "    Save within this folder the following: \n",
    "    1. A loss_acc_plot.png that is the training and validation loss vs. epochs\n",
    "      and the training and validation accuracy vs. epochs graphs.\n",
    "    2. model_history.csv that contains the training and validation loss and\n",
    "      accuracy per epoch data.\n",
    "    3. model_weights.h5 that contains the saved model.\n",
    "    \n",
    "    Args:\n",
    "      numNeurons: the number of hidden neurons used in this experiment.\n",
    "      currHistory: the current history object used to create (and then save) the CSV and plot files.\n",
    "      currModel: the current model to save.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create model output directory\n",
    "    model_name = \"model_1H\" + str(numNeurons) + \"N_lr\" + str(0.0003) + \"_bs\" + str(32) + \"_e\" + str(10)\n",
    "    MODEL_DIR = dir0_HN+model_name\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "\n",
    "    ## Save the Output\n",
    "\n",
    "    # Utilize os.path.join to add the output files to the MODEL_DIR defined above.\n",
    "    history_file = os.path.join(MODEL_DIR, 'model_history.csv')\n",
    "    plot_file = os.path.join(MODEL_DIR, 'loss_acc_plot.png')\n",
    "    model_file = os.path.join(MODEL_DIR, 'model_weights.h5')\n",
    "\n",
    "    # save the history into a CSV file\n",
    "    history_df = pd.DataFrame(currHistory.history)\n",
    "    history_df.to_csv(history_file, index=False)\n",
    "\n",
    "    # save the plots using the toolbox function and then add a title\n",
    "    combine_loss_acc_plots(currHistory, plot_loss, plot_acc, show=False)\n",
    "    plt.suptitle(model_name, fontsize=15)\n",
    "    plt.savefig(plot_file)\n",
    "\n",
    "    # save the model\n",
    "    currModel.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1H = #TODO\n",
    "model_1H_history = model_1H.fit(train_features,\n",
    "                                train_L_onehot,\n",
    "                                epochs=10, batch_size=32,\n",
    "                                validation_data=(val_features, val_L_onehot),\n",
    "                                verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs\n",
    "#saveOutputs_HN(#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"sec-Model_Tuning_Methods\"></a>\n",
    "## 4 Model Tuning Methods\n",
    "\n",
    "Now that we have built and trained the baseline neural network model, we will run a variety of experiments using different combinations of *hyperparameters*, in order to find the best performing model.\n",
    "A secondary goal is to investigate how increasing or decreasing a hyperparameter affects the accuracy of the model.\n",
    "Below is a list of hyperparameters that could be interesting to explore; feel free to experiment with your own ideas as well.\n",
    "\n",
    "We will use the `NN_Model_1H` with 18 neurons in the hidden layer as a baseline.\n",
    "Starting from this model, let us: \n",
    "\n",
    "- val with different numbers of neurons in the hidden layer: **12**, **8**, **4**, **2**, **1**\n",
    "    - It is also worthwhile to val a higher number of neurons: **40**, **80**, or more\n",
    "- val with different learning rates: **0.0003**, **0.001**, **0.01**, **0.1**\n",
    "- val with different batch sizes: **16**, **32**, **64**, **128**, **512**, **1024**\n",
    "- val with different numbers of hidden layers: **2**, **3**, and so on\n",
    "\n",
    "> **NOTE:**\n",
    "> The easiest way to do this exploration is to simply copy the code cell where we constructed and trained the baseline model and paste it to a new cell below, since most of the parameters (`hidden_neurons`, `learning_rate`, `batch_size`, etc.) can be changed when calling the `NN_Model_1H` function or when fitting the model.\n",
    "> However, to change the number of hidden layers (which we will do much later), the original `NN_model_1H` function must be duplicated and modified.\n",
    "\n",
    "#### Post-Processing\n",
    "\n",
    "To take advantage of Jupyter Notebook's ability to immediately inspect graphical \n",
    "elements, part of the post-processing will be done after each model's run.\n",
    "Inspect the resulting loss and accuracy graphs and answer the following.\n",
    "\n",
    "**QUESTIONS**: Based on the plots shown above (for the baseline model), inspect whether the training runs went as expected.\n",
    "\n",
    "1) Visually inspect for any anomalies. Note the runs that produce \"abonrmal training trends\", i.e., where the \"loss vs. epochs\" and/or \"accuracy vs. epochs\" curves exhibit a different behavior from what shown for the baseline model.\n",
    "\n",
    "2) Visually (or numerically) check for convergence (e.g. check the loss or accuracy for the last 4-5 epochs; what their slopes look like in this region; any fluctuations?)\n",
    "\n",
    "3) Observe the differences in the *final* accuracies as a result of different `hidden_neurons` values. (We will do this more carefully in the next phase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Tuning Experiments #1: Varying Number of Neurons in Hidden Layers\n",
    "\n",
    "In this round of experiments, we create several variants of `NN_Model_1H` models with varying the `hidden_neurons` hyperparameter, i.e. the number of neurons in the hidden layer.\n",
    "The accuracy and loss of each model will be assessed as a function of `hidden_neurons`.\n",
    "All the other hyperparameters (e.g. learning rate, epochs, batch_size, number of hidden layers) will be kept constant; they will be varied later.\n",
    "Not every number of hidden neurons is tested, so feel free to create new code cells with a different number of neurons as your curiousity leads you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H12N\": 12 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 12 neurons in the hidden layer\"\"\";\n",
    "\n",
    "#model_1H12N = NN_Model_1H(#TODO...)\n",
    "#model_1H12N_history = model_1H12N.fit(#TODO...)\n",
    "\n",
    "# Also plot the loss & accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H8N\": 8 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 8 neurons in the hidden layer\"\"\";\n",
    "\n",
    "#model_1H8N = NN_Model_1H(#TODO...)\n",
    "#model_1H8N_history = #TODO\n",
    "\n",
    "# Also plot the loss & accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Tips & Tricks for Experimental Runs\n",
    ">\n",
    "> Do you see the systematic names of the model and history variables, etc?\n",
    "> The variable called `model_1H12N` means \"a model with one hidden layer (`1H`) that has 12 neurons (`12N`)\".\n",
    "> The use of systematic names, albeit complicated, will be very helpful in keeping track of different experiments.\n",
    "> For example, down below, we will have models with two hidden layers; such a model can be denoted by a variable name such as `model_2H18N12N`, etc.\n",
    ">\n",
    "> **DISCUSSION QUESTION:**\n",
    "> Why don't we just name the variables `model1`, `model2`, `model3`, ...?\n",
    "> What are the advantages and disadvantages of naming them with this schema?\n",
    ">\n",
    "> **Keeping track of experimental results**:\n",
    "> At this stage, it may be helpful to keep track the final training accuracy (at the last epoch) for each model with a distinct `hidden_neurons` value.\n",
    "> You can use pen-and-paper, or build a spreadsheet with the following\n",
    "> values:\n",
    ">\n",
    "> | `hidden_neurons` | `val_accuracy` |\n",
    "> |------------------|----------------|\n",
    "> |        1         |      ....      |\n",
    "> |       ...        |      ....      |\n",
    "> |       18         | 0.9792 (example) |\n",
    "> |       ...        |      ....      |\n",
    "> |       80         |      ....      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISES**: create additional code cells to run models with 4, 2, 1 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H4N\": 4 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 4 neurons in the hidden layer\"\"\";\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H2N\": 2 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 2 neurons in the hidden layer\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H1N\": 1 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 1 neurons in the hidden layer\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**EXERCISES**: create more code cells to run models with 40 and 80 neurons in the hidden layer. *You are welcome to explore even higher numbers of hidden neurons. Observe carefully what happening!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H40N\": 40 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 40 neurons in the hidden layer\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H80N\": 80 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 80 neurons in the hidden layer\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Processing for Experiment Type 1: Hidden Neurons\n",
    "\n",
    "##### Visual inspection of graphs:\n",
    "\n",
    "Recall the post-processing questions listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Processing (Compiling the CSV) for Experiment 1: Hidden Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Method 1: Via a temporary data structure*\n",
    " \n",
    "In this method, we will construct and fill a temporary data structure (`all_lastEpochMetrics`) dynamically before forming the dataframe.\n",
    "This approach is useful when the size of the data (e.g. total number of rows) is not known *a priori*.\n",
    "\n",
    "The following is a *simplified* loop which shows the logic\n",
    "of this intermediate data construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer directory\n",
    "dirPathHN = dir0_HN\n",
    "\n",
    "# The number of neurons for each experiment/model\n",
    "listHN = [1, 2, 4, 8, 12, 18, 40, 80]\n",
    "\n",
    "# Number of epochs - 1\n",
    "lastEpochNum = 9 \n",
    "\n",
    "# Initalize. This will hold the list of dictionaries of last epoch metrics\n",
    "# (loss, val_loss, accuracy, val_accuracy)\n",
    "all_lastEpochMetrics = []\n",
    "\n",
    "# Fill in the rows for the DataFrame\n",
    "for HN in listHN:\n",
    "    # Read the history CSV file and get the last row's data, which corresponds to the last epoch data.\n",
    "    # run_subdir = \"model_1H\" + str(HN) + \"N_lr0.0003_bs32_e10\"\n",
    "    # result_csv = os.path.join(dirPathHN, run_subdir, \"model_history.csv\")\n",
    "    result_csv = fn_out_history_1H(dirPathHN, HN, 0.0003, 32, 10)\n",
    "    print(\"Reading:\", result_csv)\n",
    "    epochMetrics = pd.read_csv(result_csv)\n",
    "    # Fetch the loss, accuracy, val_loss, and val_accuracy from the last epoch\n",
    "    # (should be the last row in the CSV file unless there's something wrong\n",
    "    # during the traning)\n",
    "    lastEpochMetrics = epochMetrics.iloc[lastEpochNum, :].to_dict()\n",
    "    # Attach the \"neurons\" value\n",
    "    lastEpochMetrics[\"hidden_neurons\"] = HN\n",
    "    all_lastEpochMetrics.append(lastEpochMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the `df_HN` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HN = pd.DataFrame(all_lastEpochMetrics, \n",
    "                     columns=[\"hidden_neurons\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_HN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the post-processing results for the hidden neurons experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HN.to_csv(\"post_processing_neurons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Tuning Experiment #2: Varying Learning Rate\n",
    "\n",
    "In this batch of experiment, the accuracy and loss function of each model will be compared while changing the 'learning rate'.\n",
    "For simplicity, all the other parameters (e.g. the number of neurons, epochs, batch_size, hidden layers) will be kept constant.\n",
    "The one hidden layer with 18 neurons model will be used.\n",
    "Not every number of learning rate is included, so feel free to create new code cells with a different learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the outer learning_rate directory\n",
    "dir0_LR = \"scan-learning-rate/\"\n",
    "if not os.path.exists(dir0_LR):\n",
    "        os.makedirs(dir0_LR)\n",
    "\n",
    "## Helper function\n",
    "\n",
    "def saveOutputs_LR(learning_rate, currHistory, currModel):\n",
    "    \"\"\"\n",
    "    Save the outputs of the learning rate model tuning experiments.\n",
    "    It will create a directory within the learning rate directory with the \n",
    "    MODEL_DIR name.\n",
    "    Save within this folder the following: \n",
    "    1. A loss_acc_plot.png that is the training and validation loss vs. epochs\n",
    "      and the training and validation accuracy vs. epochs graphs.\n",
    "    2. model_history.csv that contains the training and validation loss and\n",
    "      accuracy per epoch data.\n",
    "    3. model_weights.h5 that contains the saved model.\n",
    "    \n",
    "    Args:\n",
    "      learning_rate: the learning rate used in this experiment.\n",
    "      currHistory: the current history object used to create (and then save) the CSV and plot files.\n",
    "      currModel: the current model to save.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create model output directory\n",
    "    model_name = \"model_1H18N_lr\" + str(learning_rate) + \"_bs\" + str(32) + \"_e\" + str(10)\n",
    "    MODEL_DIR = dir0_LR+model_name\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "\n",
    "    ## Save the Output\n",
    "\n",
    "    # Utilize os.path.join to add the output files to the MODEL_DIR defined above.\n",
    "    history_file = os.path.join(MODEL_DIR, 'model_history.csv')\n",
    "    plot_file = os.path.join(MODEL_DIR, 'loss_acc_plot.png')\n",
    "    model_file = os.path.join(MODEL_DIR, 'model_weights.h5')\n",
    "\n",
    "    # save the history into a CSV file\n",
    "    history_df = pd.DataFrame(currHistory.history)\n",
    "    history_df.to_csv(history_file, index=False)\n",
    "\n",
    "    # save the plots using the toolbox function and then add a title\n",
    "    combine_loss_acc_plots(currHistory, plot_loss, plot_acc, show=False)\n",
    "    plt.suptitle(model_name, fontsize=15)\n",
    "    plt.savefig(plot_file)\n",
    "\n",
    "    # save the model\n",
    "    currModel.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Learning Rate 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & learning rate=0.0003\"\"\";\n",
    "\n",
    "#model_1H18N_LR0_0003 = NN_Model_1H(#TODO...)\n",
    "#model_1H18N_LR0_0003_history = #TODO\n",
    "\n",
    "# Also plot the loss & accuracy (optional)\n",
    "\n",
    "# Save the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "\n",
    "... (create additional code cells to run models (`1H18N`) with larger learning rates: **0.001**, **0.01**,**0.1**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Learning Rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & learning rate=0.001\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Learning Rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & learning rate=0.01\"\"\";\n",
    "\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Learning Rate 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & learning rate=0.1\"\"\";\n",
    "\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Processing for Experiment Type 2: Learning Rate\n",
    "\n",
    "##### Visual inspection of graphs:\n",
    "\n",
    "Recall the post-processing questions listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Processing (Compiling the CSV) for Experiment Type 2: Learning Rate\n",
    "\n",
    "This follows the same format as the hidden neurons with different variable names.\n",
    "See above for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer directory\n",
    "dirPathLR = dir0_LR\n",
    "\n",
    "# The learning rates for each experiment/model\n",
    "listLR = [.0003, 0.001, 0.01, 0.1]\n",
    "\n",
    "# Number of epochs - 1\n",
    "lastEpochNum = 9 \n",
    "\n",
    "# Initalize. This will hold the list of dictionaries of last epoch metrics\n",
    "# (loss, val_loss, accuracy, val_accuracy)\n",
    "all_lastEpochMetrics_LR = []\n",
    "\n",
    "# Fill in the rows for the DataFrame\n",
    "for LR in listLR:\n",
    "    # Read the history CSV file and get the last row's data, which corresponds to the last epoch data.\n",
    "    # run_subdir = \"model_1H18N_lr\"+str(LR)+\"_bs32_e10\"\n",
    "    # result_csv = os.path.join(dirPathHN, run_subdir, \"model_history.csv\")\n",
    "    result_csv = fn_out_history_1H(dirPathLR, 18, LR, 32, lastEpochNum+1)\n",
    "    print(\"Reading:\", result_csv)\n",
    "    epochMetrics = pd.read_csv(result_csv)\n",
    "    # Fetch the loss, accuracy, val_loss, and val_accuracy from the last epoch\n",
    "    # (should be the last row in the CSV file unless there's something wrong\n",
    "    # during the traning)\n",
    "    lastEpochMetrics = epochMetrics.iloc[lastEpochNum, :].to_dict()\n",
    "    # Attach the \"learning rate\" value\n",
    "    lastEpochMetrics[\"learning_rate\"] = LR\n",
    "    all_lastEpochMetrics_LR.append(lastEpochMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the `df_LR` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LR = pd.DataFrame(all_lastEpochMetrics_LR, \n",
    "                     columns=[\"learning_rate\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the post-processing results for the learning rate experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LR.to_csv(\"post_processing_lr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3 Tuning Experiments #3: Varying Batch Size\n",
    "\n",
    "The accuracy and loss of each model will be compared while changing the 'batch size'.\n",
    "For simplicity, all other parameters (e.g. learning rate, epochs, number of neurons, hidden layers) will be kept constant.\n",
    "The one hidden layer with 18 neurons model will be used.\n",
    "Not every number of batch size is included, so feel free to create new code cells with a different number of batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the outer batch_size directory\n",
    "dir0_BS = \"scan-batch-size\"\n",
    "if not os.path.exists(dir0_BS):\n",
    "        os.makedirs(dir0_BS)\n",
    "\n",
    "## Helper function\n",
    "\n",
    "def saveOutputs_BS(batch_size, currHistory, currModel):\n",
    "    \"\"\"\n",
    "    Save the outputs of the batch size model tuning experiments.\n",
    "    It will create a directory within the batch size directory with the \n",
    "    MODEL_DIR name.\n",
    "    Save within this folder the following: \n",
    "    1. A loss_acc_plot.png that is the training and validation loss vs. epochs\n",
    "      and the training and validation accuracy vs. epochs graphs.\n",
    "    2. model_history.csv that contains the training and validation loss and\n",
    "      accuracy per epoch data.\n",
    "    3. model_weights.h5 that contains the saved model.\n",
    "    \n",
    "    Args:\n",
    "      batch_size: the batch size used in this experiment.\n",
    "      currHistory: the current history object used to create (and then save) the CSV and plot files.\n",
    "      currModel: the current model to save.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create model output directory\n",
    "    model_name = \"model_1H18N_lr0.0003_bs\" + str(batch_size) + \"_e\" + str(10)\n",
    "    MODEL_DIR = \"model_tuning/batch_size/\"+model_name\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "\n",
    "    ## Save the Output\n",
    "\n",
    "    # Utilize os.path.join to add the output files to the MODEL_DIR defined above.\n",
    "    history_file = os.path.join(MODEL_DIR, 'model_history.csv')\n",
    "    plot_file = os.path.join(MODEL_DIR, 'loss_acc_plot.png')\n",
    "    model_file = os.path.join(MODEL_DIR, 'model_weights.h5')\n",
    "\n",
    "    # save the history into a CSV file\n",
    "    history_df = pd.DataFrame(currHistory.history)\n",
    "    history_df.to_csv(history_file, index=False)\n",
    "\n",
    "    # save the plots using the toolbox function and then add a title\n",
    "    combine_loss_acc_plots(currHistory, plot_loss, plot_acc, show=False)\n",
    "    plt.suptitle(model_name, fontsize=15)\n",
    "    plt.savefig(plot_file)\n",
    "\n",
    "    # save the model\n",
    "    currModel.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "\n",
    "... (create additional code cells to run models (`1H18N`) with larger batch sizes, e.g. 16, 32, 64, 128, 512, 1024, ...).\n",
    "Remember that we have the original batch_size=16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Batch Size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & batch size=16\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Batch Size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & batch size=32\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Batch Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & batch size=64\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Batch Size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & batch size=128\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Batch Size 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & batch size=512\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model \"1H18N\" With Batch Size 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct & train a NN_Model_1H with 18 neurons in the hidden layer & batch size=1024\"\"\";\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Processing for Experiment Type 3: Batch Size\n",
    "\n",
    "##### Visual inspection of graphs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Post Processing (Compiling the CSV) for Experiment Type 3: Batch Size\n",
    "\n",
    "This follows the same format as the hidden neurons with different variable names.\n",
    "See above for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer directory\n",
    "dirPathLR = dir0_BS\n",
    "\n",
    "# The batch sizes for each experiment/model\n",
    "listBS = [16, 32, 64, 128, 512, 1024]\n",
    "\n",
    "# Number of epochs - 1\n",
    "lastEpochNum = 9 \n",
    "\n",
    "# Initalize. This will hold the list of dictionaries of last epoch metrics\n",
    "# (loss, val_loss, accuracy, val_accuracy)\n",
    "all_lastEpochMetrics_BS = []\n",
    "\n",
    "# Fill in the rows for the DataFrame\n",
    "for BS in listBS:\n",
    "    # Read the history CSV file and get the last row's data, which corresponds to the last epoch data.\n",
    "    # run_subdir = \"model_1H18N_lr0.0003_bs\"+str(BS)+\"_e10\"\n",
    "    # result_csv = os.path.join(dirPathHN, run_subdir, \"model_history.csv\")\n",
    "    result_csv = fn_out_history_1H(dirPathLR, 18, 0.0003, BS, lastEpochNum+1)\n",
    "    print(\"Reading:\", result_csv)\n",
    "    epochMetrics = pd.read_csv(result_csv)\n",
    "    # Fetch the loss, accuracy, val_loss, and val_accuracy from the last epoch\n",
    "    # (should be the last row in the CSV file unless there's something wrong\n",
    "    # during the traning)\n",
    "    lastEpochMetrics = epochMetrics.iloc[lastEpochNum, :].to_dict()\n",
    "    # Attach the \"batch size\" value\n",
    "    lastEpochMetrics[\"batch_size\"] = BS\n",
    "    all_lastEpochMetrics_BS.append(lastEpochMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the `df_BS` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BS = pd.DataFrame(all_lastEpochMetrics_BS, \n",
    "                     columns=[\"batch_size\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Save the post-processing results for the batch size experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BS.to_csv(\"post_processing_bs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.4 Tuning Experiments #4: Varying the number of hidden layers\n",
    "\n",
    "The accuracy and loss of each model will be compared while changing the 'number of hidden layers'.\n",
    "For simplicity, all other parameters (e.g. learning rate, epochs, batch_size, number of neurons) will be kept constant.\n",
    "Not every number of hidden layers is included, so feel free to create new code cells with a different number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the outer layers directory\n",
    "dir0_HL = \"scan-layers\"\n",
    "if not os.path.exists(dir0_HL):\n",
    "        os.makedirs(dir0_HL)\n",
    "\n",
    "## Helper function\n",
    "\n",
    "def saveOutputs_HL(num_layers, currHistory, currModel):\n",
    "    \"\"\"\n",
    "    Save the outputs of the multiple hidden layers model tuning experiments.\n",
    "    It will create a directory within the layers directory with the \n",
    "    MODEL_DIR name.\n",
    "    Save within this folder the following: \n",
    "    1. A loss_acc_plot.png that is the training and validation loss vs. epochs\n",
    "      and the training and validation accuracy vs. epochs graphs.\n",
    "    2. model_history.csv that contains the training and validation loss and\n",
    "      accuracy per epoch data.\n",
    "    3. model_weights.h5 that contains the saved model.\n",
    "    \n",
    "    Args:\n",
    "      num_layers: the number of hidden layers used in this experiment.\n",
    "      currHistory: the current history object used to create (and then save) the CSV and plot files.\n",
    "      currModel: the current model to save.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create model output directory\n",
    "    titleAdd = str(num_layers) + \"H\"\n",
    "    for j in range(num_layers):\n",
    "        titleAdd += \"18N\"\n",
    "    model_name = \"model_\"+titleAdd+\"_lr0.0003_bs32\" + \"_e\" + str(10)\n",
    "    MODEL_DIR = \"model_tuning/layers/\"+model_name\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "\n",
    "    ## Save the Output\n",
    "\n",
    "    # Utilize os.path.join to add the output files to the MODEL_DIR defined above.\n",
    "    history_file = os.path.join(MODEL_DIR, 'model_history.csv')\n",
    "    plot_file = os.path.join(MODEL_DIR, 'loss_acc_plot.png')\n",
    "    model_file = os.path.join(MODEL_DIR, 'model_weights.h5')\n",
    "\n",
    "    # save the history into a CSV file\n",
    "    history_df = pd.DataFrame(currHistory.history)\n",
    "    history_df.to_csv(history_file, index=False)\n",
    "\n",
    "    # save the plots using the toolbox function and then add a title\n",
    "    combine_loss_acc_plots(currHistory, plot_loss, plot_acc, show=False)\n",
    "    plt.suptitle(model_name, fontsize=15)\n",
    "    plt.savefig(plot_file)\n",
    "\n",
    "    # save the model\n",
    "    currModel.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the NN_Model_2H function that will build and compile a model with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_Model_2H(hidden_neurons_1,sec_hidden_neurons_1, learning_rate):\n",
    "    \"\"\"Definition of deep learning model with two dense hidden layers\"\"\"\n",
    "    random_normal_init = tf.random_normal_initializer(mean=0.0, stddev=0.05)\n",
    "    model = Sequential([\n",
    "        # More hidden layers can be added here\n",
    "        Dense(hidden_neurons_1, activation='relu', input_shape=(19,),\n",
    "              kernel_initializer=random_normal_init), # Hidden Layer\n",
    "        #TODO: Add another hidden layer\n",
    "        Dense(18, activation='softmax',\n",
    "              kernel_initializer=random_normal_init)  # Output Layer\n",
    "    ])\n",
    "    adam_opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(optimizer=adam_opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model with 18 neurons in both of the hidden layers\n",
    "model_2H18N18N = #TODO\n",
    "model_2H18N18N_history = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: save outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the NN_Model_3H function that will build and compile a model with 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_Model_3H(hidden_neurons_1,hidden_neurons_2, hidden_neurons_3, learning_rate):\n",
    "    \"\"\"Definition of deep learning model with three dense hidden layers\"\"\"\n",
    "    random_normal_init = tf.random_normal_initializer(mean=0.0, stddev=0.05)\n",
    "    model = Sequential([\n",
    "        # More hidden layers can be added here\n",
    "        Dense(hidden_neurons_1, activation='relu', input_shape=(19,),\n",
    "              kernel_initializer=random_normal_init), # Hidden Layer\n",
    "        Dense(hidden_neurons_2, activation='relu',\n",
    "              kernel_initializer=random_normal_init), # Hidden Layer\n",
    "        #TODO: Add another hidden layer\n",
    "        Dense(18, activation='softmax',\n",
    "              kernel_initializer=random_normal_init)  # Output Layer\n",
    "    ])\n",
    "    adam_opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(optimizer=adam_opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model with 18 neurons in each of the 3 hidden layers \n",
    "model_3H18N18N18N = #TODO\n",
    "model_3H18N18N18N_history = model_3H18N18N18N.fit(train_features,\n",
    "                                      train_L_onehot,\n",
    "                                      epochs=10, batch_size=32,\n",
    "                                      validation_data=(val_features, val_L_onehot),\n",
    "                                      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The NN_1H model (the model with 1 hidden layer)\n",
    "##### For simplicity sake, we will just save the output from the baseline model defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 1H18N model\n",
    "saveOutputs_HL(1, model_1H_history, model_1H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Processing for Experiment Type 4: Number of Hidden Layers\n",
    "\n",
    "##### Visual inspection of graphs:\n",
    "\n",
    "All of them follow the usual trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Post Processing (Compiling the CSV) for Experiment Type 4: Multiple Hidden Layers\n",
    "\n",
    "This follows the same format as the hidden neurons with different variable names.\n",
    "See above for more information. However, this requires a helper function located in the `sherlock_ML_toolbox.py` file called `model_layer_code_XH()`. Test it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out model_layer_code_XH(), which provides follows the naming convention.\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer directory\n",
    "dirPathHL = dir0_HL\n",
    "\n",
    "# The hidden layers for each experiment/model.\n",
    "# The number of hidden neurons in each layer input as a list.\n",
    "# Each number in the list is the number of neurons in that layer.\n",
    "listHL = [[18], [18, 18], [18, 18, 18]]\n",
    "\n",
    "# Number of epochs - 1\n",
    "lastEpochNum = 9 \n",
    "\n",
    "# Initalize. This will hold the list of dictionaries of last epoch metrics\n",
    "# (loss, val_loss, accuracy, val_accuracy)\n",
    "all_lastEpochMetrics_HL = []\n",
    "\n",
    "# Fill in the rows for the DataFrame\n",
    "for HL in listHL:\n",
    "    # Read the history CSV file and get the last row's data, which corresponds to the last epoch data.\n",
    "    result_csv = fn_out_history_XH(dirPathHL, HL, 0.0003, 32, lastEpochNum+1)\n",
    "    print(\"Reading:\", result_csv)\n",
    "    epochMetrics = pd.read_csv(result_csv)\n",
    "    # Fetch the loss, accuracy, val_loss, and val_accuracy from the last epoch\n",
    "    # (should be the last row in the CSV file unless there's something wrong\n",
    "    # during the traning)\n",
    "    lastEpochMetrics = epochMetrics.iloc[lastEpochNum, :].to_dict()\n",
    "    # Attach the \"layers\" value\n",
    "    lastEpochMetrics[\"neurons\"] = HL\n",
    "    all_lastEpochMetrics_HL.append(lastEpochMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the `df_HL` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HL = pd.DataFrame(all_lastEpochMetrics_HL, \n",
    "                     columns=[\"neurons\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_HL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Save the post-processing results for the hidden layer experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HL.to_csv(\"post_processing_layers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Tuning Opportunities\n",
    "\n",
    "There are other hyperparameters that can be adjusted:\n",
    "\n",
    "  * Change the optimizer (try optimizers other than `Adam`)\n",
    "  * Activation function  (this is actually a part of the network's architecture)\n",
    "\n",
    "We encourage you to explore the effects of changing these in your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Closing Remarks\n",
    "\n",
    "This process of experimentation with different parameters for the neural network can get repetitive and cause this notebook to become very long.\n",
    "Instead, it would be more beneficial to run experiments like this in a scripting environment.\n",
    "To do this, we need to identify the relevant code elements for our script.\n",
    "In a general sense, this is what we should pick out:\n",
    "\n",
    "* Useful Python libraries & user-defined functions\n",
    "* Proper sequence of commands that were run throughout this notebook (i.e. one-hot encoding must be done before training the models)\n",
    "* Code cells that require repetition\n",
    "\n",
    "In brief, once the initial experiments are done and we have established a working pipeline for machine learning, we need to change the way we work.\n",
    "Real machine learning work requires many repetitive experiments, each of which may take a long time to complete.\n",
    "Instead of running many experiments in Jupyter notebooks, where each will require us to wait for a while to finish, we need to be able to carry out many experiments in parallel so that we can obtain our results in a timely manner.\n",
    "This is key reason why we should make a script for these experiments and submit the script to run them in batch (non-interactive model).\n",
    "HPC is well suited for this type of workflow--in fact it is most efficient when used in this way.\n",
    "Here are the key components of the \"batch\" way of working:\n",
    "\n",
    "* A job scheduler (such as SLURM job scheduler on HPC) to manage our jobs and run them on the appropriate resources;\n",
    "* The machine learning script written in Python, which will read inputs from files and write outputs to files and/or standard output;\n",
    "* The job script to launch the machine learning script in the non-interactive environment (e.g. HPC compute node);\n",
    "* A way to systematically repeat the experiments with some variations. This can be done by adding some command-line arguments for the (hyper)parameters that will be varied for each experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
