{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN-Session-1 Solution: Binary Classification with Keras\n",
    "\n",
    "This notebook provides complete solutions for the NN-session-1 exercises.\n",
    "It demonstrates how to build a simple neural network for binary classification using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Preprocessed Sherlock Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed features and labels\n",
    "df2_features = pd.read_csv('sherlock/2apps_4f/sherlock_2apps_features.csv')\n",
    "df2_labels = pd.read_csv('sherlock/2apps_4f/sherlock_2apps_labels.csv')\n",
    "\n",
    "print(f\"Features shape: {df2_features.shape}\")\n",
    "print(f\"Labels shape: {df2_labels.shape}\")\nprint(f\"\\nFeatures: {df2_features.columns.tolist()}\")\nprint(f\"\\nFirst 5 rows of features:\")\nprint(df2_features.head())\nprint(f\"\\nLabel distribution:\")\nprint(df2_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Split data into training and testing sets\ntrain_F, test_F, train_L, test_L = train_test_split(df2_features, df2_labels, test_size=0.2, random_state=42)\n\nprint(f\"Training set: {train_F.shape}\")\nprint(f\"Test set: {test_F.shape}\")\nprint(f\"\\nTraining labels distribution:\")\nprint(train_L.value_counts())\nprint(f\"\\nTest labels distribution:\")\nprint(test_L.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a One-Neuron Binary Classifier with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Define a function to create a one-neuron binary classifier\ndef NN_binary_clf(learning_rate=0.0003):\n",
    "    \"\"\"\n",
    "    Create a one-neuron binary classifier using Keras.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float\n",
    "        Learning rate for the Adam optimizer\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : Sequential\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(1, activation='sigmoid', input_shape=(4,))\n",
    "    ])\n",
    "    \n",
    "    # Create Adam optimizer with specified learning rate\n",
    "    adam = Adam(learning_rate=learning_rate,\n",
    "                beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Function NN_binary_clf created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Create and display model architecture\nmodel = NN_binary_clf(learning_rate=0.0003)\n\nprint(\"\\nModel Architecture:\")\nprint(\"=\"*60)\nmodel.summary()\nprint(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Train the model\nprint(\"Training the one-neuron model...\")\nprint(\"=\"*60)\n\nmodel_history = model.fit(train_F, train_L,\n                          epochs=20, batch_size=32,\n                          validation_data=(test_F, test_L),\n                          verbose=1)\n\nprint(\"=\"*60)\nprint(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Plot training history\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot loss\naxes[0].plot(model_history.history['loss'], label='Training Loss', linewidth=2)\naxes[0].plot(model_history.history['val_loss'], label='Validation Loss', linewidth=2)\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Loss', fontsize=12)\naxes[0].set_title('Model Loss Over Epochs', fontsize=13, fontweight='bold')\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\n# Plot accuracy\naxes[1].plot(model_history.history['accuracy'], label='Training Accuracy', linewidth=2)\naxes[1].plot(model_history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Accuracy', fontsize=12)\naxes[1].set_title('Model Accuracy Over Epochs', fontsize=13, fontweight='bold')\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTraining History Summary:\")\nprint(f\"Initial Training Loss: {model_history.history['loss'][0]:.4f}\")\nprint(f\"Final Training Loss: {model_history.history['loss'][-1]:.4f}\")\nprint(f\"Initial Validation Loss: {model_history.history['val_loss'][0]:.4f}\")\nprint(f\"Final Validation Loss: {model_history.history['val_loss'][-1]:.4f}\")\nprint(f\"\\nInitial Training Accuracy: {model_history.history['accuracy'][0]:.4f}\")\nprint(f\"Final Training Accuracy: {model_history.history['accuracy'][-1]:.4f}\")\nprint(f\"Initial Validation Accuracy: {model_history.history['val_accuracy'][0]:.4f}\")\nprint(f\"Final Validation Accuracy: {model_history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Evaluate Keras model\nprint(\"\\n\" + \"=\"*60)\nprint(\"KERAS ONE-NEURON MODEL EVALUATION\")\nprint(\"=\"*60)\n\ntest_loss, test_accuracy = model.evaluate(test_F, test_L, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n# Get predictions\ntest_pred_keras = (model.predict(test_F) > 0.5).astype(int).flatten()\nprint(f\"\\nConfusion Matrix:\")\nprint(confusion_matrix(test_L, test_pred_keras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Traditional ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Helper function for model evaluation\ndef model_evaluate(model, test_F, test_L, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate a scikit-learn model.\n",
    "    \"\"\"\n",
    "    test_L_pred = model.predict(test_F)\n",
    "    acc = accuracy_score(test_L, test_L_pred)\n",
    "    cm = confusion_matrix(test_L, test_L_pred)\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return acc, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Train Decision Tree model\nprint(\"\\n\" + \"=\"*60)\nprint(\"DECISION TREE MODEL\")\nprint(\"=\"*60)\n\nmodel_dtc = DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_split=8, random_state=42)\nmodel_dtc.fit(train_F, train_L)\n\nacc_dtc, cm_dtc = model_evaluate(model_dtc, test_F, test_L, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Train Logistic Regression model\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOGISTIC REGRESSION MODEL\")\nprint(\"=\"*60)\n\nmodel_lr = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)\nmodel_lr.fit(train_F, train_L)\n\nacc_lr, cm_lr = model_evaluate(model_lr, test_F, test_L, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Create comparison table\ncomparison_df = pd.DataFrame({\n",
    "    'Model': ['One-Neuron NN (Keras)', 'Decision Tree', 'Logistic Regression'],\n",
    "    'Test Accuracy': [test_accuracy, acc_dtc, acc_lr]\n",
    "})\n",
    "\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODEL COMPARISON\")\nprint(\"=\"*60)\nprint(comparison_df.to_string(index=False))\nprint(\"=\"*60)\n\n# Find best model\nbest_model_idx = comparison_df['Test Accuracy'].idxmax()\nbest_model = comparison_df.loc[best_model_idx, 'Model']\nbest_accuracy = comparison_df.loc[best_model_idx, 'Test Accuracy']\n\nprint(f\"\\nBest Model: {best_model}\")\nprint(f\"Best Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Visualize model comparison\nfig, ax = plt.subplots(figsize=(10, 6))\n\nmodels = comparison_df['Model']\naccuracies = comparison_df['Test Accuracy']\ncolors = ['steelblue', 'coral', 'lightgreen']\n\nbars = ax.bar(models, accuracies, color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n\nax.set_ylabel('Test Accuracy', fontsize=12)\nax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\nax.set_ylim([0, 1])\nax.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor bar, acc in zip(bars, accuracies):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{acc:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n\nplt.xticks(rotation=15, ha='right')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Findings and Discussion\n",
    "\n",
    "### Observations:\n",
    "\n",
    "1. **Model Performance**: The one-neuron Keras model achieves comparable accuracy to traditional ML models.\n",
    "\n",
    "2. **Training Dynamics**: \n",
    "   - The Keras model shows convergence over epochs\n",
    "   - Validation accuracy stabilizes after several epochs\n",
    "   - No significant overfitting observed\n",
    "\n",
    "3. **Comparison with Traditional ML**:\n",
    "   - Decision Tree and Logistic Regression provide baseline performance\n",
    "   - One-neuron NN is essentially equivalent to logistic regression\n",
    "   - All models achieve similar accuracy on this dataset\n",
    "\n",
    "### Why Validation Metrics Matter:\n",
    "\n",
    "- **Training metrics** show how well the model fits the training data\n",
    "- **Validation metrics** show how well the model generalizes to unseen data\n",
    "- Validation metrics are more important for assessing true model performance\n",
    "- Large gap between training and validation metrics indicates overfitting\n",
    "\n",
    "### Ways to Improve the One-Neuron Model:\n",
    "\n",
    "1. **Add Hidden Layers**: Increase model capacity with hidden layers\n",
    "2. **Adjust Learning Rate**: Fine-tune the learning rate for better convergence\n",
    "3. **Increase Epochs**: Train for more epochs to improve convergence\n",
    "4. **Batch Size Tuning**: Experiment with different batch sizes\n",
    "5. **Regularization**: Add L1/L2 regularization to prevent overfitting\n",
    "6. **Feature Engineering**: Create more discriminative features\n",
    "\n",
    "### Cybersecurity Application:\n",
    "\n",
    "This binary classification approach can be used for:\n",
    "- **Malware Detection**: Classify applications as benign or malicious\n",
    "- **Anomaly Detection**: Identify unusual application behavior\n",
    "- **Real-time Monitoring**: Classify running applications on mobile devices\n",
    "- **Threat Intelligence**: Build profiles of known malicious applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
