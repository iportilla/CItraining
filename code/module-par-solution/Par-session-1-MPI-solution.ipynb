{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Par-Session-1 Solution: Introduction to MPI (Message Passing Interface)\n",
    "\n",
    "This notebook provides complete solutions for the Par-session-1 exercises.\n",
    "It demonstrates parallel programming concepts using MPI4Py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Computing Environment Setup\n",
      "Python version: 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:07:49) [Clang 20.1.8 ]\n",
      "NumPy version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Note: MPI4Py requires MPI to be installed on the system\n",
    "# This notebook demonstrates MPI concepts\n",
    "\n",
    "print(\"Parallel Computing Environment Setup\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MPI Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MPI FUNDAMENTALS ===\n",
      "\n",
      "MPI (Message Passing Interface) is a standardized communication protocol for parallel computing.\n",
      "\n",
      "Key Concepts:\n",
      "1. Communicator: A group of processes that can communicate\n",
      "2. Rank: Unique identifier for each process (0 to size-1)\n",
      "3. Size: Total number of processes\n",
      "4. Point-to-Point Communication: Send/Receive between two processes\n",
      "5. Collective Communication: Operations involving all processes\n",
      "\n",
      "Common MPI Operations:\n",
      "- Send/Recv: Point-to-point communication\n",
      "- Bcast: Broadcast data from one process to all\n",
      "- Scatter: Distribute data from one process to all\n",
      "- Gather: Collect data from all processes to one\n",
      "- Reduce: Combine data from all processes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Basic MPI concepts\n",
    "print(\"\\n=== MPI FUNDAMENTALS ===\")\n",
    "print(\"\"\"\n",
    "MPI (Message Passing Interface) is a standardized communication protocol for parallel computing.\n",
    "\n",
    "Key Concepts:\n",
    "1. Communicator: A group of processes that can communicate\n",
    "2. Rank: Unique identifier for each process (0 to size-1)\n",
    "3. Size: Total number of processes\n",
    "4. Point-to-Point Communication: Send/Receive between two processes\n",
    "5. Collective Communication: Operations involving all processes\n",
    "\n",
    "Common MPI Operations:\n",
    "- Send/Recv: Point-to-point communication\n",
    "- Bcast: Broadcast data from one process to all\n",
    "- Scatter: Distribute data from one process to all\n",
    "- Gather: Collect data from all processes to one\n",
    "- Reduce: Combine data from all processes\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MPI4Py Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HELLO WORLD MPI PROGRAM ===\n",
      "\n",
      "from mpi4py import MPI\n",
      "\n",
      "# Get communicator\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "size = comm.Get_size()\n",
      "\n",
      "print(f\"Hello from process {rank} of {size}\")\n",
      "\n",
      "\n",
      "To run: mpirun -np 4 python hello_world.py\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Example MPI program structure\n",
    "mpi_hello_world = \"\"\"\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Get communicator\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(f\"Hello from process {rank} of {size}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== HELLO WORLD MPI PROGRAM ===\")\n",
    "print(mpi_hello_world)\n",
    "print(\"\\nTo run: mpirun -np 4 python hello_world.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== POINT-TO-POINT COMMUNICATION ===\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy as np\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "size = comm.Get_size()\n",
      "\n",
      "if rank == 0:\n",
      "    # Process 0 sends data\n",
      "    data = np.array([1, 2, 3, 4, 5])\n",
      "    comm.Send(data, dest=1)\n",
      "    print(f\"Process {rank} sent data: {data}\")\n",
      "elif rank == 1:\n",
      "    # Process 1 receives data\n",
      "    data = np.empty(5, dtype=int)\n",
      "    comm.Recv(data, source=0)\n",
      "    print(f\"Process {rank} received data: {data}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Point-to-point communication example\n",
    "point_to_point = \"\"\"\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    # Process 0 sends data\n",
    "    data = np.array([1, 2, 3, 4, 5])\n",
    "    comm.Send(data, dest=1)\n",
    "    print(f\"Process {rank} sent data: {data}\")\n",
    "elif rank == 1:\n",
    "    # Process 1 receives data\n",
    "    data = np.empty(5, dtype=int)\n",
    "    comm.Recv(data, source=0)\n",
    "    print(f\"Process {rank} received data: {data}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== POINT-TO-POINT COMMUNICATION ===\")\n",
    "print(point_to_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COLLECTIVE COMMUNICATION ===\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy as np\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "size = comm.Get_size()\n",
      "\n",
      "# Broadcast: Send from rank 0 to all\n",
      "if rank == 0:\n",
      "    data = np.array([1, 2, 3, 4, 5])\n",
      "else:\n",
      "    data = np.empty(5, dtype=int)\n",
      "\n",
      "comm.Bcast(data, root=0)\n",
      "print(f\"Process {rank} received: {data}\")\n",
      "\n",
      "# Gather: Collect from all to rank 0\n",
      "local_data = np.array([rank, rank+1, rank+2])\n",
      "if rank == 0:\n",
      "    gathered = np.empty((size, 3), dtype=int)\n",
      "else:\n",
      "    gathered = None\n",
      "\n",
      "comm.Gather(local_data, gathered, root=0)\n",
      "if rank == 0:\n",
      "    print(f\"Gathered data:\n",
      "{gathered}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Collective communication example\n",
    "collective_comm = \"\"\"\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Broadcast: Send from rank 0 to all\n",
    "if rank == 0:\n",
    "    data = np.array([1, 2, 3, 4, 5])\n",
    "else:\n",
    "    data = np.empty(5, dtype=int)\n",
    "\n",
    "comm.Bcast(data, root=0)\n",
    "print(f\"Process {rank} received: {data}\")\n",
    "\n",
    "# Gather: Collect from all to rank 0\n",
    "local_data = np.array([rank, rank+1, rank+2])\n",
    "if rank == 0:\n",
    "    gathered = np.empty((size, 3), dtype=int)\n",
    "else:\n",
    "    gathered = None\n",
    "\n",
    "comm.Gather(local_data, gathered, root=0)\n",
    "if rank == 0:\n",
    "    print(f\"Gathered data:\\n{gathered}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== COLLECTIVE COMMUNICATION ===\")\n",
    "print(collective_comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parallel Computing Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MASTER-WORKER PATTERN ===\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy as np\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "size = comm.Get_size()\n",
      "\n",
      "if rank == 0:\n",
      "    # Master process\n",
      "    print(\"Master process starting...\")\n",
      "    for i in range(1, size):\n",
      "        # Send work to worker processes\n",
      "        work = np.array([i*10, i*20, i*30])\n",
      "        comm.Send(work, dest=i)\n",
      "        print(f\"Sent work to process {i}\")\n",
      "\n",
      "    # Receive results\n",
      "    for i in range(1, size):\n",
      "        result = np.empty(3, dtype=int)\n",
      "        comm.Recv(result, source=i)\n",
      "        print(f\"Received result from process {i}: {result}\")\n",
      "else:\n",
      "    # Worker process\n",
      "    work = np.empty(3, dtype=int)\n",
      "    comm.Recv(work, source=0)\n",
      "    print(f\"Process {rank} received work: {work}\")\n",
      "\n",
      "    # Process work\n",
      "    result = work * 2\n",
      "    comm.Send(result, dest=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Master-Worker pattern\n",
    "master_worker = \"\"\"\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    # Master process\n",
    "    print(\"Master process starting...\")\n",
    "    for i in range(1, size):\n",
    "        # Send work to worker processes\n",
    "        work = np.array([i*10, i*20, i*30])\n",
    "        comm.Send(work, dest=i)\n",
    "        print(f\"Sent work to process {i}\")\n",
    "    \n",
    "    # Receive results\n",
    "    for i in range(1, size):\n",
    "        result = np.empty(3, dtype=int)\n",
    "        comm.Recv(result, source=i)\n",
    "        print(f\"Received result from process {i}: {result}\")\n",
    "else:\n",
    "    # Worker process\n",
    "    work = np.empty(3, dtype=int)\n",
    "    comm.Recv(work, source=0)\n",
    "    print(f\"Process {rank} received work: {work}\")\n",
    "    \n",
    "    # Process work\n",
    "    result = work * 2\n",
    "    comm.Send(result, dest=0)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== MASTER-WORKER PATTERN ===\")\n",
    "print(master_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA PARALLELISM PATTERN ===\n",
      "\n",
      "from mpi4py import MPI\n",
      "import numpy as np\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "size = comm.Get_size()\n",
      "\n",
      "# Create data on rank 0\n",
      "if rank == 0:\n",
      "    data = np.arange(100)\n",
      "else:\n",
      "    data = None\n",
      "\n",
      "# Scatter data to all processes\n",
      "local_data = np.empty(100 // size, dtype=int)\n",
      "comm.Scatter(data, local_data, root=0)\n",
      "\n",
      "# Each process computes on its portion\n",
      "local_result = local_data ** 2\n",
      "\n",
      "# Gather results back to rank 0\n",
      "if rank == 0:\n",
      "    result = np.empty(100, dtype=int)\n",
      "else:\n",
      "    result = None\n",
      "\n",
      "comm.Gather(local_result, result, root=0)\n",
      "\n",
      "if rank == 0:\n",
      "    print(f\"Final result: {result}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Data parallelism pattern\n",
    "data_parallelism = \"\"\"\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Create data on rank 0\n",
    "if rank == 0:\n",
    "    data = np.arange(100)\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "# Scatter data to all processes\n",
    "local_data = np.empty(100 // size, dtype=int)\n",
    "comm.Scatter(data, local_data, root=0)\n",
    "\n",
    "# Each process computes on its portion\n",
    "local_result = local_data ** 2\n",
    "\n",
    "# Gather results back to rank 0\n",
    "if rank == 0:\n",
    "    result = np.empty(100, dtype=int)\n",
    "else:\n",
    "    result = None\n",
    "\n",
    "comm.Gather(local_result, result, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"Final result: {result}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== DATA PARALLELISM PATTERN ===\")\n",
    "print(data_parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERFORMANCE CONSIDERATIONS ===\n",
      "\n",
      "1. Communication Overhead:\n",
      "   - Network latency and bandwidth\n",
      "   - Message size affects performance\n",
      "   - Collective operations can be optimized\n",
      "\n",
      "2. Load Balancing:\n",
      "   - Distribute work evenly across processes\n",
      "   - Avoid idle processes\n",
      "   - Consider dynamic load balancing\n",
      "\n",
      "3. Scalability:\n",
      "   - Strong scaling: Fixed problem, increase processes\n",
      "   - Weak scaling: Increase problem and processes proportionally\n",
      "   - Amdahl's Law: Speedup limited by serial portion\n",
      "\n",
      "4. Optimization Strategies:\n",
      "   - Minimize communication\n",
      "   - Use non-blocking operations\n",
      "   - Overlap computation and communication\n",
      "   - Use efficient collective operations\n",
      "\n",
      "5. Debugging:\n",
      "   - Use MPI profiling tools\n",
      "   - Check for deadlocks\n",
      "   - Verify message ordering\n",
      "   - Use debugging flags\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Performance analysis\n",
    "print(\"\\n=== PERFORMANCE CONSIDERATIONS ===\")\n",
    "print(\"\"\"\n",
    "1. Communication Overhead:\n",
    "   - Network latency and bandwidth\n",
    "   - Message size affects performance\n",
    "   - Collective operations can be optimized\n",
    "\n",
    "2. Load Balancing:\n",
    "   - Distribute work evenly across processes\n",
    "   - Avoid idle processes\n",
    "   - Consider dynamic load balancing\n",
    "\n",
    "3. Scalability:\n",
    "   - Strong scaling: Fixed problem, increase processes\n",
    "   - Weak scaling: Increase problem and processes proportionally\n",
    "   - Amdahl's Law: Speedup limited by serial portion\n",
    "\n",
    "4. Optimization Strategies:\n",
    "   - Minimize communication\n",
    "   - Use non-blocking operations\n",
    "   - Overlap computation and communication\n",
    "   - Use efficient collective operations\n",
    "\n",
    "5. Debugging:\n",
    "   - Use MPI profiling tools\n",
    "   - Check for deadlocks\n",
    "   - Verify message ordering\n",
    "   - Use debugging flags\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Concepts Summary\n",
    "\n",
    "### MPI Basics:\n",
    "- **Communicator**: Group of processes\n",
    "- **Rank**: Process identifier\n",
    "- **Size**: Number of processes\n",
    "\n",
    "### Communication Types:\n",
    "- **Point-to-Point**: Send/Recv between two processes\n",
    "- **Collective**: Operations involving all processes\n",
    "- **Blocking**: Wait for operation to complete\n",
    "- **Non-blocking**: Continue while operation proceeds\n",
    "\n",
    "### Common Operations:\n",
    "- Send/Recv: Direct communication\n",
    "- Bcast: Broadcast from one to all\n",
    "- Scatter: Distribute from one to all\n",
    "- Gather: Collect from all to one\n",
    "- Reduce: Combine data from all\n",
    "- AllReduce: Reduce and broadcast result\n",
    "\n",
    "### Parallel Patterns:\n",
    "- Master-Worker: Centralized control\n",
    "- Data Parallelism: Distribute data\n",
    "- Pipeline: Sequential stages\n",
    "- Hybrid: Combine multiple patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cb-test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
