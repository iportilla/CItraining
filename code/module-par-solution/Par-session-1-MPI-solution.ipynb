{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Par-Session-1 Solution: Introduction to MPI (Message Passing Interface)\n",
    "\n",
    "This notebook provides complete solutions for the Par-session-1 exercises.\n",
    "It demonstrates parallel programming concepts using MPI4Py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Note: MPI4Py requires MPI to be installed on the system\n",
    "# This notebook demonstrates MPI concepts\n",
    "\n",
    "print(\"Parallel Computing Environment Setup\")\nprint(f\"Python version: {sys.version}\")\nprint(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MPI Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Basic MPI concepts\nprint(\"\\n=== MPI FUNDAMENTALS ===\")\nprint(\"\"\"\nMPI (Message Passing Interface) is a standardized communication protocol for parallel computing.\n\nKey Concepts:\n1. Communicator: A group of processes that can communicate\n2. Rank: Unique identifier for each process (0 to size-1)\n3. Size: Total number of processes\n4. Point-to-Point Communication: Send/Receive between two processes\n5. Collective Communication: Operations involving all processes\n\nCommon MPI Operations:\n- Send/Recv: Point-to-point communication\n- Bcast: Broadcast data from one process to all\n- Scatter: Distribute data from one process to all\n- Gather: Collect data from all processes to one\n- Reduce: Combine data from all processes\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MPI4Py Examples"
   ]
  },
  {
   "cell_type": "code">
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Example MPI program structure\nmpi_hello_world = \"\"\"\nfrom mpi4py import MPI\n\n# Get communicator\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\nprint(f\"Hello from process {rank} of {size}\")\n\"\"\"\n\nprint(\"\\n=== HELLO WORLD MPI PROGRAM ===\")\nprint(mpi_hello_world)\nprint(\"\\nTo run: mpirun -np 4 python hello_world.py\")"
   ]
  },
  {
   "cell_type": "code">
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Point-to-point communication example\npoint_to_point = \"\"\"\nfrom mpi4py import MPI\nimport numpy as np\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\nif rank == 0:\n    # Process 0 sends data\n    data = np.array([1, 2, 3, 4, 5])\n    comm.Send(data, dest=1)\n    print(f\"Process {rank} sent data: {data}\")\nelif rank == 1:\n    # Process 1 receives data\n    data = np.empty(5, dtype=int)\n    comm.Recv(data, source=0)\n    print(f\"Process {rank} received data: {data}\")\n\"\"\"\n\nprint(\"\\n=== POINT-TO-POINT COMMUNICATION ===\")\nprint(point_to_point)"
   ]
  },
  {
   "cell_type": "code">
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Collective communication example\ncollective_comm = \"\"\"\nfrom mpi4py import MPI\nimport numpy as np\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\n# Broadcast: Send from rank 0 to all\nif rank == 0:\n    data = np.array([1, 2, 3, 4, 5])\nelse:\n    data = np.empty(5, dtype=int)\n\ncomm.Bcast(data, root=0)\nprint(f\"Process {rank} received: {data}\")\n\n# Gather: Collect from all to rank 0\nlocal_data = np.array([rank, rank+1, rank+2])\nif rank == 0:\n    gathered = np.empty((size, 3), dtype=int)\nelse:\n    gathered = None\n\ncomm.Gather(local_data, gathered, root=0)\nif rank == 0:\n    print(f\"Gathered data:\\n{gathered}\")\n\"\"\"\n\nprint(\"\\n=== COLLECTIVE COMMUNICATION ===\")\nprint(collective_comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parallel Computing Patterns"
   ]
  },
  {
   "cell_type": "code">
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Master-Worker pattern\nmaster_worker = \"\"\"\nfrom mpi4py import MPI\nimport numpy as np\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\nif rank == 0:\n    # Master process\n    print(\"Master process starting...\")\n    for i in range(1, size):\n        # Send work to worker processes\n        work = np.array([i*10, i*20, i*30])\n        comm.Send(work, dest=i)\n        print(f\"Sent work to process {i}\")\n    \n    # Receive results\n    for i in range(1, size):\n        result = np.empty(3, dtype=int)\n        comm.Recv(result, source=i)\n        print(f\"Received result from process {i}: {result}\")\nelse:\n    # Worker process\n    work = np.empty(3, dtype=int)\n    comm.Recv(work, source=0)\n    print(f\"Process {rank} received work: {work}\")\n    \n    # Process work\n    result = work * 2\n    comm.Send(result, dest=0)\n\"\"\"\n\nprint(\"\\n=== MASTER-WORKER PATTERN ===\")\nprint(master_worker)"
   ]
  },
  {
   "cell_type": "code">
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Data parallelism pattern\ndata_parallelism = \"\"\"\nfrom mpi4py import MPI\nimport numpy as np\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nsize = comm.Get_size()\n\n# Create data on rank 0\nif rank == 0:\n    data = np.arange(100)\nelse:\n    data = None\n\n# Scatter data to all processes\nlocal_data = np.empty(100 // size, dtype=int)\ncomm.Scatter(data, local_data, root=0)\n\n# Each process computes on its portion\nlocal_result = local_data ** 2\n\n# Gather results back to rank 0\nif rank == 0:\n    result = np.empty(100, dtype=int)\nelse:\n    result = None\n\ncomm.Gather(local_result, result, root=0)\n\nif rank == 0:\n    print(f\"Final result: {result}\")\n\"\"\"\n\nprint(\"\\n=== DATA PARALLELISM PATTERN ===\")\nprint(data_parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Considerations"
   ]
  },
  {
   "cell_type": "code">
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Performance analysis\nprint(\"\\n=== PERFORMANCE CONSIDERATIONS ===\")\nprint(\"\"\"\n1. Communication Overhead:\n   - Network latency and bandwidth\n   - Message size affects performance\n   - Collective operations can be optimized\n\n2. Load Balancing:\n   - Distribute work evenly across processes\n   - Avoid idle processes\n   - Consider dynamic load balancing\n\n3. Scalability:\n   - Strong scaling: Fixed problem, increase processes\n   - Weak scaling: Increase problem and processes proportionally\n   - Amdahl's Law: Speedup limited by serial portion\n\n4. Optimization Strategies:\n   - Minimize communication\n   - Use non-blocking operations\n   - Overlap computation and communication\n   - Use efficient collective operations\n\n5. Debugging:\n   - Use MPI profiling tools\n   - Check for deadlocks\n   - Verify message ordering\n   - Use debugging flags\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Concepts Summary\n",
    "\n",
    "### MPI Basics:\n",
    "- **Communicator**: Group of processes\n",
    "- **Rank**: Process identifier\n",
    "- **Size**: Number of processes\n",
    "\n",
    "### Communication Types:\n",
    "- **Point-to-Point**: Send/Recv between two processes\n",
    "- **Collective**: Operations involving all processes\n",
    "- **Blocking**: Wait for operation to complete\n",
    "- **Non-blocking**: Continue while operation proceeds\n",
    "\n",
    "### Common Operations:\n",
    "- Send/Recv: Direct communication\n",
    "- Bcast: Broadcast from one to all\n",
    "- Scatter: Distribute from one to all\n",
    "- Gather: Collect from all to one\n",
    "- Reduce: Combine data from all\n",
    "- AllReduce: Reduce and broadcast result\n",
    "\n",
    "### Parallel Patterns:\n",
    "- Master-Worker: Centralized control\n",
    "- Data Parallelism: Distribute data\n",
    "- Pipeline: Sequential stages\n",
    "- Hybrid: Combine multiple patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
