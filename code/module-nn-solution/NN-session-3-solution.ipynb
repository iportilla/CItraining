{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN-Session-3 Solution: Advanced Neural Network Tuning\n",
    "\n",
    "This notebook provides complete solutions for the NN-session-3 exercises.\n",
    "It demonstrates advanced techniques for tuning neural networks to achieve high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Load and preprocess 18-apps dataset\ndf = pd.read_csv(\"sherlock/sherlock_18apps.csv\", index_col=0)\n\n# Data cleaning\ndf2 = df.copy()\ndf2 = df2.drop(['Unnamed: 0'], axis=1, errors='ignore')\ndf2.dropna(inplace=True)\n\n# Separate labels and features\nlabels = df2['ApplicationName']\ndf_features = df2.drop('ApplicationName', axis=1)\n\n# Feature scaling\nscaler = preprocessing.StandardScaler()\nscaler.fit(df_features)\ndf_features_n = pd.DataFrame(scaler.transform(df_features),\n                             columns=df_features.columns,\n                             index=df_features.index)\n\n# One-hot encoding\ndf_labels_onehot = pd.get_dummies(labels)\ndf_features_encoded = pd.get_dummies(df_features_n)\n\n# Train-test split\ntrain_F, test_F, train_L, test_L = train_test_split(\n    df_features_encoded, df_labels_onehot, test_size=0.2, random_state=42\n)\n\nprint(f\"Training set: {train_F.shape}\")\nprint(f\"Test set: {test_F.shape}\")\nprint(f\"Number of classes: {train_L.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Define function to create models with different architectures\ndef create_model(hidden_layers=[128, 64, 32], dropout_rate=0.2, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Create a neural network with specified architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hidden_layers : list\n",
    "        Number of units in each hidden layer\n",
    "    dropout_rate : float\n",
    "        Dropout rate for regularization\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer and first hidden layer\n",
    "    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(train_F.shape[1],)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Additional hidden layers\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(train_L.shape[1], activation='softmax'))\n",
    "    \n",
    "    # Compile\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Model creation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Baseline model\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 1: BASELINE MODEL\")\nprint(\"=\"*60)\n\nmodel_baseline = create_model(hidden_layers=[128, 64, 32], dropout_rate=0.2, learning_rate=0.001)\n\nprint(\"\\nModel Architecture:\")\nmodel_baseline.summary()\n\nhistory_baseline = model_baseline.fit(\n    train_F, train_L,\n    epochs=50, batch_size=32,\n    validation_data=(test_F, test_L),\n    verbose=0\n)\n\ntest_loss, test_acc = model_baseline.evaluate(test_F, test_L, verbose=0)\nprint(f\"\\nBaseline Model Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Deeper Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Deeper network\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 2: DEEPER NETWORK\")\nprint(\"=\"*60)\n\nmodel_deep = create_model(hidden_layers=[256, 128, 64, 32], dropout_rate=0.3, learning_rate=0.001)\n\nhistory_deep = model_deep.fit(\n    train_F, train_L,\n    epochs=50, batch_size=32,\n    validation_data=(test_F, test_L),\n    verbose=0\n)\n\ntest_loss, test_acc_deep = model_deep.evaluate(test_F, test_L, verbose=0)\nprint(f\"Deeper Network Accuracy: {test_acc_deep:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Wider Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Wider network\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 3: WIDER NETWORK\")\nprint(\"=\"*60)\n\nmodel_wide = create_model(hidden_layers=[512, 256, 128], dropout_rate=0.2, learning_rate=0.001)\n\nhistory_wide = model_wide.fit(\n    train_F, train_L,\n    epochs=50, batch_size=32,\n    validation_data=(test_F, test_L),\n    verbose=0\n)\n\ntest_loss, test_acc_wide = model_wide.evaluate(test_F, test_L, verbose=0)\nprint(f\"Wider Network Accuracy: {test_acc_wide:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Different Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Different learning rate\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 4: DIFFERENT LEARNING RATE\")\nprint(\"=\"*60)\n\nmodel_lr = create_model(hidden_layers=[256, 128, 64], dropout_rate=0.2, learning_rate=0.0005)\n\nhistory_lr = model_lr.fit(\n    train_F, train_L,\n    epochs=50, batch_size=32,\n    validation_data=(test_F, test_L),\n    verbose=0\n)\n\ntest_loss, test_acc_lr = model_lr.evaluate(test_F, test_L, verbose=0)\nprint(f\"Lower Learning Rate Accuracy: {test_acc_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: With Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Model with early stopping\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 5: WITH EARLY STOPPING\")\nprint(\"=\"*60)\n\nmodel_es = create_model(hidden_layers=[256, 128, 64, 32], dropout_rate=0.2, learning_rate=0.001)\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n\nhistory_es = model_es.fit(\n    train_F, train_L,\n    epochs=100, batch_size=32,\n    validation_data=(test_F, test_L),\n    callbacks=[early_stop, reduce_lr],\n    verbose=0\n)\n\ntest_loss, test_acc_es = model_es.evaluate(test_F, test_L, verbose=0)\nprint(f\"With Early Stopping Accuracy: {test_acc_es:.4f}\")\nprint(f\"Epochs trained: {len(history_es.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Create comprehensive comparison\nresults = pd.DataFrame({\n    'Experiment': [\n        'Baseline (128-64-32)',\n        'Deeper (256-128-64-32)',\n        'Wider (512-256-128)',\n        'Lower LR (0.0005)',\n        'Early Stopping'\n    ],\n    'Architecture': [\n        '[128, 64, 32]',\n        '[256, 128, 64, 32]',\n        '[512, 256, 128]',\n        '[256, 128, 64]',\n        '[256, 128, 64, 32]'\n    ],\n    'Learning Rate': [0.001, 0.001, 0.001, 0.0005, 0.001],\n    'Test Accuracy': [test_acc, test_acc_deep, test_acc_wide, test_acc_lr, test_acc_es]\n})\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPREHENSIVE RESULTS COMPARISON\")\nprint(\"=\"*80)\nprint(results.to_string(index=False))\nprint(\"=\"*80)\n\nbest_idx = results['Test Accuracy'].idxmax()\nprint(f\"\\nBest Model: {results.loc[best_idx, 'Experiment']}\")\nprint(f\"Best Accuracy: {results.loc[best_idx, 'Test Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Visualize training history comparison\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\naxes = axes.flatten()\n\nhistories = [\n    ('Baseline', history_baseline),\n    ('Deeper', history_deep),\n    ('Wider', history_wide),\n    ('Lower LR', history_lr),\n    ('Early Stopping', history_es)\n]\n\nfor idx, (name, history) in enumerate(histories):\n    ax = axes[idx]\n    ax.plot(history.history['accuracy'], label='Training', linewidth=2)\n    ax.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n    ax.set_title(f'{name}', fontweight='bold')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Accuracy')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_ylim([0, 1])\n\n# Remove extra subplot\nfig.delaxes(axes[5])\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Bar chart comparison\nfig, ax = plt.subplots(figsize=(12, 6))\n\ncolors = ['steelblue', 'coral', 'lightgreen', 'gold', 'plum']\nbars = ax.bar(range(len(results)), results['Test Accuracy'], color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n\nax.set_ylabel('Test Accuracy', fontsize=12)\nax.set_title('Neural Network Tuning: Accuracy Comparison', fontsize=14, fontweight='bold')\nax.set_xticks(range(len(results)))\nax.set_xticklabels(results['Experiment'], rotation=15, ha='right')\nax.set_ylim([0, 1])\nax.axhline(y=0.99, color='red', linestyle='--', linewidth=2, label='Target (99%)')\nax.grid(axis='y', alpha=0.3)\nax.legend()\n\nfor bar, acc in zip(bars, results['Test Accuracy']):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{acc:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Analyze impact of different hyperparameters\nprint(\"\\n\" + \"=\"*60)\nprint(\"HYPERPARAMETER IMPACT ANALYSIS\")\nprint(\"=\"*60)\n\nprint(\"\\n1. Network Depth Impact:\")\nprint(f\"   Baseline (3 layers): {test_acc:.4f}\")\nprint(f\"   Deeper (4 layers): {test_acc_deep:.4f}\")\nprint(f\"   Impact: {(test_acc_deep - test_acc)*100:+.2f}%\")\n\nprint(\"\\n2. Network Width Impact:\")\nprint(f\"   Baseline (128-64-32): {test_acc:.4f}\")\nprint(f\"   Wider (512-256-128): {test_acc_wide:.4f}\")\nprint(f\"   Impact: {(test_acc_wide - test_acc)*100:+.2f}%\")\n\nprint(\"\\n3. Learning Rate Impact:\")\nprint(f\"   Standard (0.001): {test_acc:.4f}\")\nprint(f\"   Lower (0.0005): {test_acc_lr:.4f}\")\nprint(f\"   Impact: {(test_acc_lr - test_acc)*100:+.2f}%\")\n\nprint(\"\\n4. Early Stopping Impact:\")\nprint(f\"   Without: {test_acc:.4f}\")\nprint(f\"   With: {test_acc_es:.4f}\")\nprint(f\"   Impact: {(test_acc_es - test_acc)*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings and Recommendations\n",
    "\n",
    "### Observations:\n",
    "\n",
    "1. **Network Architecture**:\n",
    "   - Deeper networks can capture more complex patterns\n",
    "   - Wider networks provide more capacity per layer\n",
    "   - Balance is key - too deep/wide can lead to overfitting\n",
    "\n",
    "2. **Learning Rate**:\n",
    "   - Lower learning rates allow finer convergence\n",
    "   - May require more epochs to converge\n",
    "   - Adaptive methods (Adam) help with convergence\n",
    "\n",
    "3. **Regularization**:\n",
    "   - Dropout prevents overfitting\n",
    "   - Batch normalization stabilizes training\n",
    "   - Early stopping prevents unnecessary training\n",
    "\n",
    "4. **Callbacks**:\n",
    "   - Early stopping saves training time\n",
    "   - Learning rate reduction helps fine-tuning\n",
    "   - Combination of callbacks improves results\n",
    "\n",
    "### Recommendations for Achieving >99% Accuracy:\n",
    "\n",
    "1. **Increase Model Capacity**: Use deeper/wider networks\n",
    "2. **Fine-tune Learning Rate**: Use adaptive learning rate schedules\n",
    "3. **Add Regularization**: Use dropout and batch normalization\n",
    "4. **Use Callbacks**: Implement early stopping and LR reduction\n",
    "5. **Data Augmentation**: Generate synthetic training data\n",
    "6. **Ensemble Methods**: Combine multiple models\n",
    "7. **Feature Engineering**: Create more discriminative features\n",
    "\n",
    "### Cybersecurity Applications:\n",
    "\n",
    "- **High-Accuracy Classification**: Critical for security systems\n",
    "- **Malware Detection**: Requires >99% accuracy to minimize false negatives\n",
    "- **Behavioral Analysis**: Detect anomalous application behavior\n",
    "- **Real-time Monitoring**: Efficient models for mobile deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
